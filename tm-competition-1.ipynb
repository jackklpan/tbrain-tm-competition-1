{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import bisect\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pd.options.display.max_rows = 4000\n",
    "pd.options.display.max_columns = 4000\n",
    "# pd.options.display.max_seq_items = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv('training-set.csv', header=None, names=['FileID', 'label'])\n",
    "test_df = pd.read_csv('testing-set.csv', header=None, names=['FileID', 'label'])\n",
    "train_exception_df = pd.read_csv('exception_train.txt', header=None, names=['FileID'])\n",
    "test_exception_df = pd.read_csv('exception_testing.txt', header=None, names=['FileID'])\n",
    "\n",
    "training_df = training_df[~training_df['FileID'].isin(train_exception_df['FileID'])]\n",
    "test_df = test_df[~test_df['FileID'].isin(test_exception_df['FileID'])]\n",
    "\n",
    "training_df = training_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(training_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "training_df['fold'] = None\n",
    "count = 0\n",
    "for train_index, test_index in skf.split(training_df.drop('label', axis=1), training_df['label']):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #print(test_index.tolist())\n",
    "    #print(train_ans_df.iloc[test_index])\n",
    "    training_df.loc[test_index, 'fold'] = count\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "five_folds = [[[0, 1, 2, 3], [4]], [[0, 1, 2, 4], [3]], [[0, 1, 3, 4], [2]], [[0, 2, 3, 4], [1]], [[1, 2, 3, 4], [0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "temp_dfs = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    temp_dfs.append(pd.read_csv('from_table/customer_virus_ratio_fold'+str(i)+'.csv').set_index('t1_CustomerID'))\n",
    "\n",
    "# print(temp_dfs[0].head())\n",
    "# print(temp_dfs[1].head())\n",
    "# print(temp_dfs[0].add(temp_dfs[1], fill_value=0).head())\n",
    "\n",
    "customer_virus_ratio_dfs = []\n",
    "customer_virus_ratio_df_groups = []\n",
    "for idx, fold in enumerate(five_folds):\n",
    "    merge_folds = fold[0]\n",
    "    temp_df = reduce(lambda left, right: left.add(right, fill_value=0), \\\n",
    "                     [temp_dfs[merge_folds[0]], temp_dfs[merge_folds[1]], temp_dfs[merge_folds[2]], temp_dfs[merge_folds[3]]])\n",
    "    temp_df['ratio'] = temp_df['virus_event_count'] / temp_df['total_event_count']\n",
    "    #print(temp_df[['virus_event_count', 'total_event_count']])\n",
    "    customer_virus_ratio_dfs.append(temp_df.reset_index())\n",
    "    customer_virus_ratio_df_groups.append(customer_virus_ratio_dfs[idx].groupby('t1_CustomerID'))\n",
    "    #print(customer_virus_ratio_dfs[idx])\n",
    "    print(idx)\n",
    "    print(fold)\n",
    "    \n",
    "\n",
    "temp_dfs = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    temp_dfs.append(pd.read_csv('from_table/product_virus_ratio_fold'+str(i)+'.csv').set_index('t1_ProductID'))\n",
    "\n",
    "# print(temp_dfs[0].head())\n",
    "# print(temp_dfs[1].head())\n",
    "# print(temp_dfs[0].add(temp_dfs[1], fill_value=0).head())\n",
    "\n",
    "product_virus_ratio_dfs = []\n",
    "product_virus_ratio_df_groups = []\n",
    "for idx, fold in enumerate(five_folds):\n",
    "    merge_folds = fold[0]\n",
    "    temp_df = reduce(lambda left, right: left.add(right, fill_value=0), \\\n",
    "                     [temp_dfs[merge_folds[0]], temp_dfs[merge_folds[1]], temp_dfs[merge_folds[2]], temp_dfs[merge_folds[3]]])\n",
    "    temp_df['ratio'] = temp_df['virus_event_count'] / temp_df['total_event_count']\n",
    "    #print(temp_df[['virus_event_count', 'total_event_count']])\n",
    "    product_virus_ratio_dfs.append(temp_df.reset_index())\n",
    "    product_virus_ratio_df_groups.append(product_virus_ratio_dfs[idx].groupby('t1_ProductID'))\n",
    "    #print(product_virus_ratio_dfs[idx])\n",
    "    print(idx)\n",
    "    print(fold)\n",
    "    \n",
    "\n",
    "temp_dfs = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    customer_virus_ratio_df = pd.read_csv('from_table/customer_virus_ratio_fold'+str(i)+'.csv')\n",
    "    customer_file_count_df = pd.read_csv('from_table/customer_file_count_fold'+str(i)+'.csv')\n",
    "    \n",
    "    customer_virus_ratio_df['t2_CustomerID'] = customer_virus_ratio_df['t1_CustomerID']\n",
    "    customer_virus_ratio_df = customer_virus_ratio_df.drop('t1_CustomerID', axis=1)\n",
    "    \n",
    "    customer_file_count_df = customer_file_count_df.merge(customer_virus_ratio_df.drop('total_event_count', axis=1), on='t2_CustomerID', how='left')\n",
    "    customer_file_count_df = customer_file_count_df.fillna(0)\n",
    "    temp_dfs.append(customer_file_count_df.set_index('t2_CustomerID'))\n",
    "\n",
    "# print(temp_dfs[0].head())\n",
    "# print(temp_dfs[1].head())\n",
    "# print(temp_dfs[0].add(temp_dfs[1], fill_value=0).head())\n",
    "\n",
    "customer_file_count_dfs = []\n",
    "for idx, fold in enumerate(five_folds):\n",
    "    merge_folds = fold[0]\n",
    "    temp_df = reduce(lambda left, right: left.add(right, fill_value=0), \\\n",
    "                     [temp_dfs[merge_folds[0]], temp_dfs[merge_folds[1]], temp_dfs[merge_folds[2]], temp_dfs[merge_folds[3]]])\n",
    "    #print(temp_df[['virus_event_count', 'total_event_count']])\n",
    "    temp_df['ratio'] = temp_df['virus_event_count'] / (temp_df['total_event_count']+.000000001)\n",
    "    customer_file_count_dfs.append(temp_df.reset_index())\n",
    "    #print(product_virus_ratio_dfs[idx])\n",
    "    print(idx)\n",
    "    print(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df['affect_user_count'] = None\n",
    "training_df['total_event_count'] = None\n",
    "training_df['each_user_event_avg'] = None\n",
    "training_df['each_user_event_std'] = None\n",
    "training_df['each_user_event_rstd'] = None\n",
    "\n",
    "training_df['first_time_occur'] = None\n",
    "training_df['last_time_occur'] = None\n",
    "training_df['time_duration'] = None\n",
    "\n",
    "training_df['first_24h_count'] = None\n",
    "training_df['first_24h_ratio'] = None\n",
    "training_df['first_24h_affect_user_count'] = None\n",
    "training_df['first_24h_affect_user_ratio'] = None\n",
    "training_df['first_24h_each_user_event_avg'] = None\n",
    "\n",
    "for i in range(0, 10):\n",
    "    training_df['bin_'+str(i)+'_count'] = None\n",
    "    training_df['bin_'+str(i)+'_ratio'] = None\n",
    "    training_df['bin_'+str(i)+'_affect_user_count'] = None\n",
    "    training_df['bin_'+str(i)+'_affect_user_ratio'] = None\n",
    "    training_df['bin_'+str(i)+'_each_user_event_avg'] = None\n",
    "\n",
    "training_df['customer_spread_time_mean'] = None\n",
    "training_df['customer_spread_time_std'] = None\n",
    "training_df['customer_spread_time_rstd'] = None\n",
    "\n",
    "training_df['event_diff_time_mean'] = None\n",
    "training_df['event_diff_time_median'] = None\n",
    "training_df['event_diff_time_std'] = None\n",
    "training_df['event_diff_time_rstd'] = None\n",
    "\n",
    "training_df['customer_virus_ratio_avg'] = None\n",
    "training_df['customer_virus_ratio_min'] = None\n",
    "training_df['customer_virus_ratio_max'] = None\n",
    "training_df['customer_virus_ratio_median'] = None\n",
    "training_df['customer_virus_ratio_std'] = None\n",
    "training_df['customer_virus_ratio_rstd'] = None  \n",
    "\n",
    "training_df['product_virus_ratio_avg'] = None\n",
    "training_df['product_virus_ratio_min'] = None\n",
    "training_df['product_virus_ratio_max'] = None\n",
    "training_df['product_virus_ratio_median'] = None\n",
    "training_df['product_virus_ratio_std'] = None\n",
    "training_df['product_virus_ratio_rstd'] = None  \n",
    "\n",
    "training_df['customer_has_static_ratio'] = None\n",
    "training_df['customer_has_static_b3_ratio'] = None\n",
    "training_df['customer_has_static_count'] = None\n",
    "training_df['customer_has_static_b3_count'] = None\n",
    "training_df['customer_virus_ratio_wavg'] = None\n",
    "training_df['customer_virus_zero_count'] = None  \n",
    "training_df['customer_virus_zero_ratio'] = None  \n",
    "training_df['customer_virus_zero_ratio2'] = None  \n",
    "\n",
    "training_df['hour_ratio_mean'] = None\n",
    "training_df['hour_ratio_median'] = None\n",
    "training_df['hour_ratio_std'] = None\n",
    "training_df['hour_ratio_rstd'] = None\n",
    "training_df['hour_ratio_max'] = None\n",
    "training_df['hour_ratio_min'] = None\n",
    "training_df['hour_occupy_ratio'] = None\n",
    "training_df['hour_mean'] = None\n",
    "\n",
    "training_df['dayofweek_ratio_mean'] = None\n",
    "training_df['dayofweek_ratio_median'] = None\n",
    "training_df['dayofweek_ratio_std'] = None\n",
    "training_df['dayofweek_ratio_rstd'] = None\n",
    "training_df['dayofweek_ratio_max'] = None\n",
    "training_df['dayofweek_ratio_min'] = None\n",
    "training_df['dayofweek_occupy_ratio'] = None\n",
    "training_df['dayofweek_mean'] = None\n",
    "\n",
    "training_df['bin_ratio_mean'] = None\n",
    "training_df['bin_ratio_median'] = None\n",
    "training_df['bin_ratio_std'] = None\n",
    "training_df['bin_ratio_rstd'] = None\n",
    "training_df['bin_ratio_max'] = None\n",
    "training_df['bin_ratio_min'] = None\n",
    "training_df['bin_occupy_ratio'] = None\n",
    "\n",
    "training_df['event_diff_time_mean2'] = None\n",
    "training_df['event_diff_time_median2'] = None\n",
    "training_df['event_diff_time_std2'] = None\n",
    "training_df['event_diff_time_rstd2'] = None\n",
    "training_df['event_diff_time_max2'] = None\n",
    "training_df['event_diff_time_min2'] = None\n",
    "training_df['event_diff_time_lessone_ratio2'] = None\n",
    "\n",
    "training_df['all_event_diff_time_mean'] = None\n",
    "training_df['all_event_diff_time_median'] = None\n",
    "training_df['all_event_diff_time_std'] = None\n",
    "training_df['all_event_diff_time_rstd'] = None\n",
    "training_df['all_event_diff_time_max'] = None\n",
    "training_df['all_event_diff_time_min'] = None\n",
    "training_df['all_event_diff_time_lessone_ratio'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in tqdm(training_df.iterrows(), total=len(training_df)):\n",
    "    temp_df = pd.read_csv('file_log/'+data['FileID']+'.csv', header=None, names=['FileID', 'CustomerID', 'QueryTS', 'ProductID'])\n",
    "    total_event_count = len(temp_df)\n",
    "    training_df.loc[idx, 'affect_user_count'] = len(temp_df['CustomerID'].unique())\n",
    "    training_df.loc[idx, 'total_event_count'] = total_event_count\n",
    "    training_df.loc[idx, 'each_user_event_avg'] = training_df.loc[idx, 'total_event_count'] / training_df.loc[idx, 'affect_user_count']\n",
    "    training_df.loc[idx, 'each_user_event_std'] = temp_df.groupby('CustomerID').count()['QueryTS'].std()\n",
    "    training_df.loc[idx, 'each_user_event_rstd'] = training_df.loc[idx, 'each_user_event_std'] / training_df.loc[idx, 'each_user_event_avg']\n",
    "    \n",
    "    time_start = temp_df['QueryTS'].min()\n",
    "    time_end = temp_df['QueryTS'].max()\n",
    "    time_range = time_end - time_start\n",
    "    \n",
    "    training_df.loc[idx, 'first_time_occur'] = time_start\n",
    "    training_df.loc[idx, 'last_time_occur'] = time_end\n",
    "    training_df.loc[idx, 'time_duration'] = time_range\n",
    "    \n",
    "    the_flag = ( (temp_df['QueryTS']>=time_start)&(temp_df['QueryTS']<=time_start+86400) )\n",
    "    the_24h_count = np.sum(the_flag)\n",
    "    training_df.loc[idx, 'first_24h_count'] = the_24h_count\n",
    "    training_df.loc[idx, 'first_24h_ratio'] = the_24h_count / total_event_count\n",
    "    training_df.loc[idx, 'first_24h_affect_user_count'] = len(temp_df[the_flag]['CustomerID'].unique())\n",
    "    training_df.loc[idx, 'first_24h_affect_user_ratio'] = training_df.loc[idx, 'first_24h_affect_user_count'] / training_df.loc[idx, 'affect_user_count']\n",
    "    training_df.loc[idx, 'first_24h_each_user_event_avg'] = the_24h_count / (training_df.loc[idx, 'first_24h_affect_user_count']+.000000001)\n",
    "    \n",
    "    time_step = time_range/10.0\n",
    "    for i in range(0, 10):\n",
    "        the_flag = ( (temp_df['QueryTS']>=time_start+time_step*i)&(temp_df['QueryTS']<time_start+time_step*(i+1)+0.5) )\n",
    "        the_bin_count = np.sum(the_flag)\n",
    "        training_df.loc[idx, 'bin_'+str(i)+'_count'] = the_bin_count\n",
    "        training_df.loc[idx, 'bin_'+str(i)+'_ratio'] = the_bin_count / total_event_count\n",
    "        training_df.loc[idx, 'bin_'+str(i)+'_affect_user_count'] = len(temp_df[the_flag]['CustomerID'].unique())\n",
    "        training_df.loc[idx, 'bin_'+str(i)+'_affect_user_ratio'] = training_df.loc[idx, 'bin_'+str(i)+'_affect_user_count'] / training_df.loc[idx, 'affect_user_count']\n",
    "        training_df.loc[idx, 'bin_'+str(i)+'_each_user_event_avg'] = the_bin_count / (training_df.loc[idx, 'bin_'+str(i)+'_affect_user_count']+.000000001)\n",
    "    \n",
    "    the_static_df = temp_df.groupby('CustomerID').min().sort_values(['QueryTS'], ascending=True)['QueryTS'].diff().fillna(0)\n",
    "    training_df.loc[idx, 'customer_spread_time_mean'] = the_static_df.mean()\n",
    "    training_df.loc[idx, 'customer_spread_time_std'] = the_static_df.std()\n",
    "    training_df.loc[idx, 'customer_spread_time_rstd'] = training_df.loc[idx, 'customer_spread_time_std'] / (training_df.loc[idx, 'customer_spread_time_mean']+.000000001)\n",
    "    \n",
    "    the_static_df = temp_df.sort_values(['CustomerID', 'QueryTS'], ascending=True).groupby(['CustomerID'])['QueryTS'].diff().fillna(0)\n",
    "    training_df.loc[idx, 'event_diff_time_mean'] = the_static_df.mean()\n",
    "    training_df.loc[idx, 'event_diff_time_median'] = the_static_df.median()\n",
    "    training_df.loc[idx, 'event_diff_time_std'] = the_static_df.std()\n",
    "    training_df.loc[idx, 'event_diff_time_rstd'] = training_df.loc[idx, 'event_diff_time_std'] / (training_df.loc[idx, 'event_diff_time_mean']+.000000001)\n",
    "    \n",
    "    temp_df['datetime'] = pd.to_datetime(temp_df['QueryTS'], unit='s')\n",
    "    temp_df['hour'] = temp_df['datetime'].dt.hour\n",
    "    temp_df['dayofweek'] = temp_df['datetime'].dt.dayofweek\n",
    "    \n",
    "    hour_ratio_df = temp_df[['FileID', 'hour']].groupby('hour').count()/temp_df.shape[0]\n",
    "    training_df.loc[idx, 'hour_ratio_mean'] = hour_ratio_df['FileID'].mean()\n",
    "    training_df.loc[idx, 'hour_ratio_median'] = hour_ratio_df['FileID'].median()\n",
    "    training_df.loc[idx, 'hour_ratio_std'] = hour_ratio_df['FileID'].std()\n",
    "    training_df.loc[idx, 'hour_ratio_rstd'] = hour_ratio_df['FileID'].std() / (hour_ratio_df['FileID'].mean()+.000000001)\n",
    "    training_df.loc[idx, 'hour_ratio_max'] = hour_ratio_df['FileID'].max()\n",
    "    training_df.loc[idx, 'hour_ratio_min'] = hour_ratio_df['FileID'].min()\n",
    "    training_df.loc[idx, 'hour_occupy_ratio'] = temp_df[['FileID', 'hour']].groupby('hour').count().shape[0]/24.0\n",
    "    training_df.loc[idx, 'hour_mean'] = temp_df['hour'].mean()\n",
    "    \n",
    "    dayofweek_ratio_df = temp_df[['FileID', 'dayofweek']].groupby('dayofweek').count()/temp_df.shape[0]\n",
    "    training_df.loc[idx, 'dayofweek_ratio_mean'] = dayofweek_ratio_df['FileID'].mean()\n",
    "    training_df.loc[idx, 'dayofweek_ratio_median'] = dayofweek_ratio_df['FileID'].median()\n",
    "    training_df.loc[idx, 'dayofweek_ratio_std'] = dayofweek_ratio_df['FileID'].std()\n",
    "    training_df.loc[idx, 'dayofweek_ratio_rstd'] = dayofweek_ratio_df['FileID'].std() / (dayofweek_ratio_df['FileID'].mean()+.000000001)\n",
    "    training_df.loc[idx, 'dayofweek_ratio_max'] = dayofweek_ratio_df['FileID'].max()\n",
    "    training_df.loc[idx, 'dayofweek_ratio_min'] = dayofweek_ratio_df['FileID'].min()\n",
    "    training_df.loc[idx, 'dayofweek_occupy_ratio'] = temp_df[['FileID', 'dayofweek']].groupby('dayofweek').count().shape[0]/7.0\n",
    "    training_df.loc[idx, 'dayofweek_mean'] = temp_df['dayofweek'].mean()\n",
    "    \n",
    "    bin_ratio_array = training_df.loc[idx, ['bin_0_ratio', 'bin_1_ratio', 'bin_2_ratio', 'bin_3_ratio', 'bin_4_ratio', \\\n",
    "                            'bin_5_ratio', 'bin_6_ratio', 'bin_7_ratio', 'bin_8_ratio', 'bin_9_ratio']].values\n",
    "    training_df.loc[idx, 'bin_ratio_mean'] = np.mean(bin_ratio_array)\n",
    "    training_df.loc[idx, 'bin_ratio_median'] = np.median(bin_ratio_array)\n",
    "    training_df.loc[idx, 'bin_ratio_std'] = np.std(bin_ratio_array)\n",
    "    training_df.loc[idx, 'bin_ratio_rstd'] = np.std(bin_ratio_array) / (np.mean(bin_ratio_array)+.000000001)\n",
    "    training_df.loc[idx, 'bin_ratio_max'] = np.max(bin_ratio_array)\n",
    "    training_df.loc[idx, 'bin_ratio_min'] = np.min(bin_ratio_array)\n",
    "    training_df.loc[idx, 'bin_occupy_ratio'] = np.sum(bin_ratio_array>0)/10.0\n",
    "    \n",
    "    the_static_df = temp_df.sort_values(['CustomerID', 'QueryTS'], ascending=True).groupby(['CustomerID'])['QueryTS'].diff().dropna()\n",
    "    if the_static_df.shape[0] > 0:\n",
    "        training_df.loc[idx, 'event_diff_time_mean2'] = the_static_df.mean()\n",
    "        training_df.loc[idx, 'event_diff_time_median2'] = the_static_df.median()\n",
    "        training_df.loc[idx, 'event_diff_time_std2'] = the_static_df.std()\n",
    "        training_df.loc[idx, 'event_diff_time_rstd2'] = training_df.loc[idx, 'event_diff_time_std2'] / (training_df.loc[idx, 'event_diff_time_mean2']+.000000001)\n",
    "        training_df.loc[idx, 'event_diff_time_max2'] = the_static_df.max()\n",
    "        training_df.loc[idx, 'event_diff_time_min2'] = the_static_df.min()\n",
    "        training_df.loc[idx, 'event_diff_time_lessone_ratio2'] = np.sum(the_static_df<=1) / len(the_static_df)\n",
    "    \n",
    "    the_static_df = temp_df.sort_values(['QueryTS'], ascending=True)['QueryTS'].diff().dropna()\n",
    "    if the_static_df.shape[0] > 0:\n",
    "        training_df.loc[idx, 'all_event_diff_time_mean'] = the_static_df.mean()\n",
    "        training_df.loc[idx, 'all_event_diff_time_median'] = the_static_df.median()\n",
    "        training_df.loc[idx, 'all_event_diff_time_std'] = the_static_df.std()\n",
    "        training_df.loc[idx, 'all_event_diff_time_rstd'] = training_df.loc[idx, 'all_event_diff_time_std'] / (training_df.loc[idx, 'all_event_diff_time_mean']+.000000001)\n",
    "        training_df.loc[idx, 'all_event_diff_time_max'] = the_static_df.max()\n",
    "        training_df.loc[idx, 'all_event_diff_time_min'] = the_static_df.min()\n",
    "        training_df.loc[idx, 'all_event_diff_time_lessone_ratio'] = np.sum(the_static_df<=1) / len(the_static_df)\n",
    "\n",
    "#     print(training_df.loc[idx])\n",
    "# #     print(temp_df)\n",
    "# #     print(len(temp_df['CustomerID'].unique()))\n",
    "# #     print(data)\n",
    "#     if idx==2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_five_folds = []\n",
    "\n",
    "for fold_idx, fold in enumerate(reversed(five_folds)):\n",
    "    customer_virus_ratio_df_group = customer_virus_ratio_df_groups[fold_idx]\n",
    "    product_virus_ratio_df_group = product_virus_ratio_df_groups[fold_idx]\n",
    "    customer_file_count_df = customer_file_count_dfs[fold_idx].sort_values('t2_CustomerID')\n",
    "    customer_file_count_array = customer_file_count_df['t2_CustomerID'].tolist()\n",
    "    customer_file_count_array.sort()\n",
    "    \n",
    "    \n",
    "    fold_training = training_df[training_df['fold'].isin(fold[1])].copy()\n",
    "    \n",
    "    for data_idx, data in tqdm(fold_training.iterrows(), total=len(fold_training)):\n",
    "        temp_df = pd.read_csv('file_log/'+data['FileID']+'.csv', header=None, names=['FileID', 'CustomerID', 'QueryTS', 'ProductID'])\n",
    "        unique_customers = temp_df['CustomerID'].unique().tolist()\n",
    "        unique_products = temp_df['ProductID'].unique().tolist()\n",
    "        \n",
    "        temp_ratio = []\n",
    "        temp_count_index = []\n",
    "        for customer_id in unique_customers:\n",
    "            try:\n",
    "                found_ratio = customer_virus_ratio_df_group.get_group(customer_id).iloc[0]\n",
    "                if found_ratio['total_event_count'] >= 3:\n",
    "                    temp_ratio.append(found_ratio['ratio'])\n",
    "            except:\n",
    "                temp_ratio.append(0)\n",
    "                \n",
    "            try:\n",
    "                found_count_index = bisect.bisect(customer_file_count_array, customer_id)\n",
    "                if customer_file_count_array[found_count_index-1] == customer_id:\n",
    "                    temp_count_index.append(found_count_index-1)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        if len(temp_ratio) > 0:        \n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_avg'] = np.mean(temp_ratio)\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_min'] = np.min(temp_ratio)\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_max'] = np.max(temp_ratio)\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_median'] = np.median(temp_ratio)\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_std'] = np.std(temp_ratio)\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_rstd'] = np.std(temp_ratio) / (np.mean(temp_ratio)+.000000001)\n",
    "        else:\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_avg'] = 0\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_min'] = 0\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_max'] = 0\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_median'] = 0\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_std'] = 0\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_rstd'] = 0\n",
    "            \n",
    "        temp_ratio = []\n",
    "        for product_id in unique_products:\n",
    "            try:\n",
    "                found_ratio = product_virus_ratio_df_group.get_group(product_id).iloc[0]\n",
    "                if found_ratio['total_event_count'] >= 3:\n",
    "                    temp_ratio.append(found_ratio['ratio'])\n",
    "            except:\n",
    "                temp_ratio.append(0)\n",
    "                \n",
    "        if len(temp_ratio) > 0:        \n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_avg'] = np.mean(temp_ratio)\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_min'] = np.min(temp_ratio)\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_max'] = np.max(temp_ratio)\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_median'] = np.median(temp_ratio)\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_std'] = np.std(temp_ratio)\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_rstd'] = np.std(temp_ratio) / (np.mean(temp_ratio)+.000000001)\n",
    "        else:\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_avg'] = 0\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_min'] = 0\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_max'] = 0\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_median'] = 0\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_std'] = 0\n",
    "            fold_training.loc[data_idx, 'product_virus_ratio_rstd'] = 0\n",
    "        \n",
    "        customer_has_static_df = customer_file_count_df.iloc[temp_count_index, :]\n",
    "        fold_training.loc[data_idx, 'customer_has_static_ratio'] = customer_has_static_df.shape[0] / len(unique_customers)\n",
    "        customer_has_static_b3_df = customer_has_static_df[customer_has_static_df['total_event_count']>=3]\n",
    "        fold_training.loc[data_idx, 'customer_has_static_b3_ratio'] = customer_has_static_b3_df.shape[0] / len(unique_customers)\n",
    "\n",
    "        fold_training.loc[data_idx, 'customer_has_static_count'] = customer_has_static_df.shape[0]\n",
    "        fold_training.loc[data_idx, 'customer_has_static_b3_count'] = customer_has_static_b3_df.shape[0]\n",
    "\n",
    "        if customer_has_static_df.shape[0] > 0:\n",
    "            fold_training.loc[data_idx, 'customer_virus_ratio_wavg'] = np.average(customer_has_static_df['ratio'], weights=customer_has_static_df['total_event_count'])            \n",
    "            fold_training.loc[data_idx, 'customer_virus_zero_count'] = np.sum(customer_has_static_df['virus_event_count']==0)\n",
    "            fold_training.loc[data_idx, 'customer_virus_zero_ratio'] = fold_training.loc[data_idx, 'customer_virus_zero_count'] / fold_training.loc[data_idx, 'affect_user_count']\n",
    "            fold_training.loc[data_idx, 'customer_virus_zero_ratio2'] = fold_training.loc[data_idx, 'customer_virus_zero_count'] / fold_training.loc[data_idx, 'customer_has_static_count']\n",
    "        \n",
    "#         print(fold_training.loc[data_idx])\n",
    "#         break\n",
    "    \n",
    "    print(fold_training.head())\n",
    "    training_five_folds.append(fold_training)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df = pd.concat([training_five_folds[0], training_five_folds[1], training_five_folds[2], training_five_folds[3], training_five_folds[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['affect_user_count'] = None\n",
    "test_df['total_event_count'] = None\n",
    "test_df['each_user_event_avg'] = None\n",
    "test_df['each_user_event_std'] = None\n",
    "test_df['each_user_event_rstd'] = None\n",
    "\n",
    "test_df['first_time_occur'] = None\n",
    "test_df['last_time_occur'] = None\n",
    "test_df['time_duration'] = None\n",
    "\n",
    "test_df['first_24h_count'] = None\n",
    "test_df['first_24h_ratio'] = None\n",
    "test_df['first_24h_affect_user_count'] = None\n",
    "test_df['first_24h_affect_user_ratio'] = None\n",
    "test_df['first_24h_each_user_event_avg'] = None\n",
    "\n",
    "for i in range(0, 10):\n",
    "    test_df['bin_'+str(i)+'_count'] = None\n",
    "    test_df['bin_'+str(i)+'_ratio'] = None\n",
    "    test_df['bin_'+str(i)+'_affect_user_count'] = None\n",
    "    test_df['bin_'+str(i)+'_affect_user_ratio'] = None\n",
    "    test_df['bin_'+str(i)+'_each_user_event_avg'] = None\n",
    "\n",
    "test_df['customer_spread_time_mean'] = None\n",
    "test_df['customer_spread_time_std'] = None\n",
    "test_df['customer_spread_time_rstd'] = None\n",
    "\n",
    "test_df['event_diff_time_mean'] = None\n",
    "test_df['event_diff_time_median'] = None\n",
    "test_df['event_diff_time_std'] = None\n",
    "test_df['event_diff_time_rstd'] = None\n",
    "\n",
    "test_df['customer_virus_ratio_avg'] = None\n",
    "test_df['customer_virus_ratio_min'] = None\n",
    "test_df['customer_virus_ratio_max'] = None\n",
    "test_df['customer_virus_ratio_median'] = None\n",
    "test_df['customer_virus_ratio_std'] = None\n",
    "test_df['customer_virus_ratio_rstd'] = None  \n",
    "\n",
    "test_df['product_virus_ratio_avg'] = None\n",
    "test_df['product_virus_ratio_min'] = None\n",
    "test_df['product_virus_ratio_max'] = None\n",
    "test_df['product_virus_ratio_median'] = None\n",
    "test_df['product_virus_ratio_std'] = None\n",
    "test_df['product_virus_ratio_rstd'] = None  \n",
    "\n",
    "test_df['customer_has_static_ratio'] = None\n",
    "test_df['customer_has_static_b3_ratio'] = None\n",
    "test_df['customer_has_static_count'] = None\n",
    "test_df['customer_has_static_b3_count'] = None\n",
    "test_df['customer_virus_ratio_wavg'] = None\n",
    "test_df['customer_virus_zero_count'] = None  \n",
    "test_df['customer_virus_zero_ratio'] = None  \n",
    "test_df['customer_virus_zero_ratio2'] = None  \n",
    "\n",
    "test_df['hour_ratio_mean'] = None\n",
    "test_df['hour_ratio_median'] = None\n",
    "test_df['hour_ratio_std'] = None\n",
    "test_df['hour_ratio_rstd'] = None\n",
    "test_df['hour_ratio_max'] = None\n",
    "test_df['hour_ratio_min'] = None\n",
    "test_df['hour_occupy_ratio'] = None\n",
    "test_df['hour_mean'] = None\n",
    "\n",
    "test_df['dayofweek_ratio_mean'] = None\n",
    "test_df['dayofweek_ratio_median'] = None\n",
    "test_df['dayofweek_ratio_std'] = None\n",
    "test_df['dayofweek_ratio_rstd'] = None\n",
    "test_df['dayofweek_ratio_max'] = None\n",
    "test_df['dayofweek_ratio_min'] = None\n",
    "test_df['dayofweek_occupy_ratio'] = None\n",
    "test_df['dayofweek_mean'] = None\n",
    "\n",
    "test_df['bin_ratio_mean'] = None\n",
    "test_df['bin_ratio_median'] = None\n",
    "test_df['bin_ratio_std'] = None\n",
    "test_df['bin_ratio_rstd'] = None\n",
    "test_df['bin_ratio_max'] = None\n",
    "test_df['bin_ratio_min'] = None\n",
    "test_df['bin_occupy_ratio'] = None\n",
    "\n",
    "test_df['event_diff_time_mean2'] = None\n",
    "test_df['event_diff_time_median2'] = None\n",
    "test_df['event_diff_time_std2'] = None\n",
    "test_df['event_diff_time_rstd2'] = None\n",
    "test_df['event_diff_time_max2'] = None\n",
    "test_df['event_diff_time_min2'] = None\n",
    "test_df['event_diff_time_lessone_ratio2'] = None\n",
    "\n",
    "test_df['all_event_diff_time_mean'] = None\n",
    "test_df['all_event_diff_time_median'] = None\n",
    "test_df['all_event_diff_time_std'] = None\n",
    "test_df['all_event_diff_time_rstd'] = None\n",
    "test_df['all_event_diff_time_max'] = None\n",
    "test_df['all_event_diff_time_min'] = None\n",
    "test_df['all_event_diff_time_lessone_ratio'] = None\n",
    "\n",
    "for idx, data in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    temp_df = pd.read_csv('file_log/'+data['FileID']+'.csv', header=None, names=['FileID', 'CustomerID', 'QueryTS', 'ProductID'])\n",
    "    total_event_count = len(temp_df)\n",
    "    test_df.loc[idx, 'affect_user_count'] = len(temp_df['CustomerID'].unique())\n",
    "    test_df.loc[idx, 'total_event_count'] = total_event_count\n",
    "    test_df.loc[idx, 'each_user_event_avg'] = test_df.loc[idx, 'total_event_count'] / test_df.loc[idx, 'affect_user_count']\n",
    "    test_df.loc[idx, 'each_user_event_std'] = temp_df.groupby('CustomerID').count()['QueryTS'].std()\n",
    "    test_df.loc[idx, 'each_user_event_rstd'] = test_df.loc[idx, 'each_user_event_std'] / test_df.loc[idx, 'each_user_event_avg']\n",
    "    \n",
    "    time_start = temp_df['QueryTS'].min()\n",
    "    time_end = temp_df['QueryTS'].max()\n",
    "    time_range = time_end - time_start\n",
    "    \n",
    "    test_df.loc[idx, 'first_time_occur'] = time_start\n",
    "    test_df.loc[idx, 'last_time_occur'] = time_end\n",
    "    test_df.loc[idx, 'time_duration'] = time_range\n",
    "    \n",
    "    the_flag = ( (temp_df['QueryTS']>=time_start)&(temp_df['QueryTS']<=time_start+86400) )\n",
    "    the_24h_count = np.sum(the_flag)\n",
    "    test_df.loc[idx, 'first_24h_count'] = the_24h_count\n",
    "    test_df.loc[idx, 'first_24h_ratio'] = the_24h_count / total_event_count\n",
    "    test_df.loc[idx, 'first_24h_affect_user_count'] = len(temp_df[the_flag]['CustomerID'].unique())\n",
    "    test_df.loc[idx, 'first_24h_affect_user_ratio'] = test_df.loc[idx, 'first_24h_affect_user_count'] / test_df.loc[idx, 'affect_user_count']\n",
    "    test_df.loc[idx, 'first_24h_each_user_event_avg'] = the_24h_count / (test_df.loc[idx, 'first_24h_affect_user_count']+.000000001)\n",
    "    \n",
    "    time_step = time_range/10.0\n",
    "    for i in range(0, 10):\n",
    "        the_flag = ( (temp_df['QueryTS']>=time_start+time_step*i)&(temp_df['QueryTS']<time_start+time_step*(i+1)+0.5) )\n",
    "        the_bin_count = np.sum(the_flag)\n",
    "        test_df.loc[idx, 'bin_'+str(i)+'_count'] = the_bin_count\n",
    "        test_df.loc[idx, 'bin_'+str(i)+'_ratio'] = the_bin_count / total_event_count\n",
    "        test_df.loc[idx, 'bin_'+str(i)+'_affect_user_count'] = len(temp_df[the_flag]['CustomerID'].unique())\n",
    "        test_df.loc[idx, 'bin_'+str(i)+'_affect_user_ratio'] = test_df.loc[idx, 'bin_'+str(i)+'_affect_user_count'] / test_df.loc[idx, 'affect_user_count']\n",
    "        test_df.loc[idx, 'bin_'+str(i)+'_each_user_event_avg'] = the_bin_count / (test_df.loc[idx, 'bin_'+str(i)+'_affect_user_count']+.000000001)\n",
    "    \n",
    "    the_static_df = temp_df.groupby('CustomerID').min().sort_values(['QueryTS'], ascending=True)['QueryTS'].diff().fillna(0)\n",
    "    test_df.loc[idx, 'customer_spread_time_mean'] = the_static_df.mean()\n",
    "    test_df.loc[idx, 'customer_spread_time_std'] = the_static_df.std()\n",
    "    test_df.loc[idx, 'customer_spread_time_rstd'] = test_df.loc[idx, 'customer_spread_time_std'] / (test_df.loc[idx, 'customer_spread_time_mean']+.000000001)\n",
    "    \n",
    "    the_static_df = temp_df.sort_values(['CustomerID', 'QueryTS'], ascending=True).groupby(['CustomerID'])['QueryTS'].diff().fillna(0)\n",
    "    test_df.loc[idx, 'event_diff_time_mean'] = the_static_df.mean()\n",
    "    test_df.loc[idx, 'event_diff_time_median'] = the_static_df.median()\n",
    "    test_df.loc[idx, 'event_diff_time_std'] = the_static_df.std()\n",
    "    test_df.loc[idx, 'event_diff_time_rstd'] = test_df.loc[idx, 'event_diff_time_std'] / (test_df.loc[idx, 'event_diff_time_mean']+.000000001)\n",
    "    \n",
    "    unique_customers = temp_df['CustomerID'].unique().tolist()\n",
    "    temp_ratio = []\n",
    "    temp_count_index = []\n",
    "    for customer_id in unique_customers:\n",
    "        try:\n",
    "            found_ratio = customer_virus_ratio_df_group.get_group(customer_id).iloc[0]\n",
    "            if found_ratio['total_event_count'] >= 3:\n",
    "                temp_ratio.append(found_ratio['ratio'])\n",
    "        except:\n",
    "            temp_ratio.append(0)\n",
    "            \n",
    "        try:\n",
    "            found_count_index = bisect.bisect(customer_file_count_array, customer_id)\n",
    "            if customer_file_count_array[found_count_index-1] == customer_id:\n",
    "                temp_count_index.append(found_count_index-1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    if len(temp_ratio) > 0:        \n",
    "        test_df.loc[idx, 'customer_virus_ratio_avg'] = np.mean(temp_ratio)\n",
    "        test_df.loc[idx, 'customer_virus_ratio_min'] = np.min(temp_ratio)\n",
    "        test_df.loc[idx, 'customer_virus_ratio_max'] = np.max(temp_ratio)\n",
    "        test_df.loc[idx, 'customer_virus_ratio_median'] = np.median(temp_ratio)\n",
    "        test_df.loc[idx, 'customer_virus_ratio_std'] = np.std(temp_ratio)\n",
    "        test_df.loc[idx, 'customer_virus_ratio_rstd'] = np.std(temp_ratio) / (np.mean(temp_ratio)+.000000001)\n",
    "    else:\n",
    "        test_df.loc[idx, 'customer_virus_ratio_avg'] = 0\n",
    "        test_df.loc[idx, 'customer_virus_ratio_min'] = 0\n",
    "        test_df.loc[idx, 'customer_virus_ratio_max'] = 0\n",
    "        test_df.loc[idx, 'customer_virus_ratio_median'] = 0\n",
    "        test_df.loc[idx, 'customer_virus_ratio_std'] = 0\n",
    "        test_df.loc[idx, 'customer_virus_ratio_rstd'] = 0\n",
    "    \n",
    "    unique_products = temp_df['ProductID'].unique().tolist()\n",
    "    temp_ratio = []\n",
    "    for product_id in unique_products:\n",
    "        try:\n",
    "            found_ratio = product_virus_ratio_df_group.get_group(product_id).iloc[0]\n",
    "            if found_ratio['total_event_count'] >= 3:\n",
    "                temp_ratio.append(found_ratio['ratio'])\n",
    "        except:\n",
    "            temp_ratio.append(0)\n",
    "\n",
    "    if len(temp_ratio) > 0:\n",
    "        test_df.loc[idx, 'product_virus_ratio_avg'] = np.mean(temp_ratio)\n",
    "        test_df.loc[idx, 'product_virus_ratio_min'] = np.min(temp_ratio)\n",
    "        test_df.loc[idx, 'product_virus_ratio_max'] = np.max(temp_ratio)\n",
    "        test_df.loc[idx, 'product_virus_ratio_median'] = np.median(temp_ratio)\n",
    "        test_df.loc[idx, 'product_virus_ratio_std'] = np.std(temp_ratio)\n",
    "        test_df.loc[idx, 'product_virus_ratio_rstd'] = np.std(temp_ratio) / (np.mean(temp_ratio)+.000000001)\n",
    "    else:\n",
    "        test_df.loc[idx, 'product_virus_ratio_avg'] = 0\n",
    "        test_df.loc[idx, 'product_virus_ratio_min'] = 0\n",
    "        test_df.loc[idx, 'product_virus_ratio_max'] = 0\n",
    "        test_df.loc[idx, 'product_virus_ratio_median'] = 0\n",
    "        test_df.loc[idx, 'product_virus_ratio_std'] = 0\n",
    "        test_df.loc[idx, 'product_virus_ratio_rstd'] = 0\n",
    "        \n",
    "    customer_has_static_df = customer_file_count_df.iloc[temp_count_index, :]\n",
    "    test_df.loc[idx, 'customer_has_static_ratio'] = customer_has_static_df.shape[0] / len(unique_customers)\n",
    "    customer_has_static_b3_df = customer_has_static_df[customer_has_static_df['total_event_count']>=3]\n",
    "    test_df.loc[idx, 'customer_has_static_b3_ratio'] = customer_has_static_b3_df.shape[0] / len(unique_customers)\n",
    "\n",
    "    test_df.loc[idx, 'customer_has_static_count'] = customer_has_static_df.shape[0]\n",
    "    test_df.loc[idx, 'customer_has_static_b3_count'] = customer_has_static_b3_df.shape[0]\n",
    "\n",
    "    if customer_has_static_df.shape[0] > 0:\n",
    "        test_df.loc[idx, 'customer_virus_ratio_wavg'] = np.average(customer_has_static_df['ratio'], weights=customer_has_static_df['total_event_count'])            \n",
    "        test_df.loc[idx, 'customer_virus_zero_count'] = np.sum(customer_has_static_df['virus_event_count']==0)\n",
    "        test_df.loc[idx, 'customer_virus_zero_ratio'] = test_df.loc[idx, 'customer_virus_zero_count'] / test_df.loc[idx, 'affect_user_count']\n",
    "        test_df.loc[idx, 'customer_virus_zero_ratio2'] = test_df.loc[idx, 'customer_virus_zero_count'] / test_df.loc[idx, 'customer_has_static_count']\n",
    "\n",
    "    temp_df['datetime'] = pd.to_datetime(temp_df['QueryTS'], unit='s')\n",
    "    temp_df['hour'] = temp_df['datetime'].dt.hour\n",
    "    temp_df['dayofweek'] = temp_df['datetime'].dt.dayofweek\n",
    "    \n",
    "    hour_ratio_df = temp_df[['FileID', 'hour']].groupby('hour').count()/temp_df.shape[0]\n",
    "    test_df.loc[idx, 'hour_ratio_mean'] = hour_ratio_df['FileID'].mean()\n",
    "    test_df.loc[idx, 'hour_ratio_median'] = hour_ratio_df['FileID'].median()\n",
    "    test_df.loc[idx, 'hour_ratio_std'] = hour_ratio_df['FileID'].std()\n",
    "    test_df.loc[idx, 'hour_ratio_rstd'] = hour_ratio_df['FileID'].std() / (hour_ratio_df['FileID'].mean()+.000000001)\n",
    "    test_df.loc[idx, 'hour_ratio_max'] = hour_ratio_df['FileID'].max()\n",
    "    test_df.loc[idx, 'hour_ratio_min'] = hour_ratio_df['FileID'].min()\n",
    "    test_df.loc[idx, 'hour_occupy_ratio'] = temp_df[['FileID', 'hour']].groupby('hour').count().shape[0]/24.0\n",
    "    test_df.loc[idx, 'hour_mean'] = temp_df['hour'].mean()\n",
    "    \n",
    "    dayofweek_ratio_df = temp_df[['FileID', 'dayofweek']].groupby('dayofweek').count()/temp_df.shape[0]\n",
    "    test_df.loc[idx, 'dayofweek_ratio_mean'] = dayofweek_ratio_df['FileID'].mean()\n",
    "    test_df.loc[idx, 'dayofweek_ratio_median'] = dayofweek_ratio_df['FileID'].median()\n",
    "    test_df.loc[idx, 'dayofweek_ratio_std'] = dayofweek_ratio_df['FileID'].std()\n",
    "    test_df.loc[idx, 'dayofweek_ratio_rstd'] = dayofweek_ratio_df['FileID'].std() / (dayofweek_ratio_df['FileID'].mean()+.000000001)\n",
    "    test_df.loc[idx, 'dayofweek_ratio_max'] = dayofweek_ratio_df['FileID'].max()\n",
    "    test_df.loc[idx, 'dayofweek_ratio_min'] = dayofweek_ratio_df['FileID'].min()\n",
    "    test_df.loc[idx, 'dayofweek_occupy_ratio'] = temp_df[['FileID', 'dayofweek']].groupby('dayofweek').count().shape[0]/7.0\n",
    "    test_df.loc[idx, 'dayofweek_mean'] = temp_df['dayofweek'].mean()\n",
    "    \n",
    "    bin_ratio_array = test_df.loc[idx, ['bin_0_ratio', 'bin_1_ratio', 'bin_2_ratio', 'bin_3_ratio', 'bin_4_ratio', \\\n",
    "                            'bin_5_ratio', 'bin_6_ratio', 'bin_7_ratio', 'bin_8_ratio', 'bin_9_ratio']].values\n",
    "    test_df.loc[idx, 'bin_ratio_mean'] = np.mean(bin_ratio_array)\n",
    "    test_df.loc[idx, 'bin_ratio_median'] = np.median(bin_ratio_array)\n",
    "    test_df.loc[idx, 'bin_ratio_std'] = np.std(bin_ratio_array)\n",
    "    test_df.loc[idx, 'bin_ratio_rstd'] = np.std(bin_ratio_array) / (np.mean(bin_ratio_array)+.000000001)\n",
    "    test_df.loc[idx, 'bin_ratio_max'] = np.max(bin_ratio_array)\n",
    "    test_df.loc[idx, 'bin_ratio_min'] = np.min(bin_ratio_array)\n",
    "    test_df.loc[idx, 'bin_occupy_ratio'] = np.sum(bin_ratio_array>0)/10.0\n",
    "    \n",
    "    the_static_df = temp_df.sort_values(['CustomerID', 'QueryTS'], ascending=True).groupby(['CustomerID'])['QueryTS'].diff().dropna()\n",
    "    if the_static_df.shape[0] > 0:\n",
    "        test_df.loc[idx, 'event_diff_time_mean2'] = the_static_df.mean()\n",
    "        test_df.loc[idx, 'event_diff_time_median2'] = the_static_df.median()\n",
    "        test_df.loc[idx, 'event_diff_time_std2'] = the_static_df.std()\n",
    "        test_df.loc[idx, 'event_diff_time_rstd2'] = test_df.loc[idx, 'event_diff_time_std2'] / (test_df.loc[idx, 'event_diff_time_mean2']+.000000001)\n",
    "        test_df.loc[idx, 'event_diff_time_max2'] = the_static_df.max()\n",
    "        test_df.loc[idx, 'event_diff_time_min2'] = the_static_df.min()\n",
    "        test_df.loc[idx, 'event_diff_time_lessone_ratio2'] = np.sum(the_static_df<=1) / len(the_static_df)\n",
    "    \n",
    "    the_static_df = temp_df.sort_values(['QueryTS'], ascending=True)['QueryTS'].diff().dropna()\n",
    "    if the_static_df.shape[0] > 0:\n",
    "        test_df.loc[idx, 'all_event_diff_time_mean'] = the_static_df.mean()\n",
    "        test_df.loc[idx, 'all_event_diff_time_median'] = the_static_df.median()\n",
    "        test_df.loc[idx, 'all_event_diff_time_std'] = the_static_df.std()\n",
    "        test_df.loc[idx, 'all_event_diff_time_rstd'] = test_df.loc[idx, 'all_event_diff_time_std'] / (test_df.loc[idx, 'all_event_diff_time_mean']+.000000001)\n",
    "        test_df.loc[idx, 'all_event_diff_time_max'] = the_static_df.max()\n",
    "        test_df.loc[idx, 'all_event_diff_time_min'] = the_static_df.min()\n",
    "        test_df.loc[idx, 'all_event_diff_time_lessone_ratio'] = np.sum(the_static_df<=1) / len(the_static_df)\n",
    "        \n",
    "#     print(test_df.loc[idx])\n",
    "# #     print(temp_df)\n",
    "# #     print(len(temp_df['CustomerID'].unique()))\n",
    "# #     print(data)\n",
    "#     if idx==2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_columns = ['bin_0_affect_user_ratio', 'first_24h_each_user_event_avg', 'bin_ratio_max', \\\n",
    "                  'customer_has_static_b3_count', 'hour_occupy_ratio', 'bin_9_affect_user_ratio', \\\n",
    "                  'dayofweek_ratio_std', 'event_diff_time_rstd', 'each_user_event_std', \\\n",
    "                  'event_diff_time_lessone_ratio2', 'bin_7_affect_user_ratio', 'first_24h_affect_user_ratio', \\\n",
    "                  'all_event_diff_time_mean', 'product_virus_ratio_max', 'customer_virus_zero_count', \\\n",
    "                  'customer_spread_time_mean', 'customer_virus_ratio_rstd', 'first_24h_affect_user_count', \\\n",
    "                  'hour_ratio_max', 'hour_ratio_std', 'customer_has_static_b3_ratio', 'first_24h_ratio', \\\n",
    "                  'customer_has_static_ratio', 'customer_has_static_count', 'event_diff_time_median', \\\n",
    "                  'hour_ratio_mean', 'first_24h_count', 'customer_virus_ratio_median', 'event_diff_time_median2', \\\n",
    "                  'all_event_diff_time_median', 'customer_virus_ratio_std', 'customer_virus_ratio_avg', \\\n",
    "                  'customer_virus_ratio_max', 'each_user_event_avg', 'customer_virus_zero_ratio', \\\n",
    "                  'customer_virus_zero_ratio2', 'customer_virus_ratio_wavg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'l2', 'auc'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'lambda_l2': 5.0,\n",
    "    'min_gain_to_split': 0,\n",
    "    'min_data_in_leaf': 8,\n",
    "    'max_depth': 15\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(training_df[useful_columns].values, training_df['label'].values)\n",
    "lgb_eval = lgb.Dataset(training_df[useful_columns].values, training_df['label'].values, reference=lgb_train)\n",
    "gbm = lgb.train(params,\n",
    "            lgb_train,\n",
    "            num_boost_round=105,\n",
    "            valid_sets=lgb_eval)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(training_df['label'], gbm.predict(training_df[useful_columns].values, num_iteration=gbm.best_iteration))\n",
    "print(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1_df['lgbm'] = gbm.predict(training_df[useful_columns].values, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(training_df[useful_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_results = []\n",
    "clssfis = []\n",
    "for idx, fold in enumerate(five_folds):\n",
    "    merge_folds = fold[0]\n",
    "    fold_training = training_df[training_df['fold'].isin(merge_folds)]\n",
    "\n",
    "    clssfi = MLPClassifier(random_state=0, hidden_layer_sizes=(10, 3, 3), alpha=0.2)\n",
    "    clssfi.fit(scaler.transform(fold_training[useful_columns]), fold_training['label'])\n",
    "    fpr, tpr, thresholds = roc_curve(fold_training['label'], clssfi.predict_proba(scaler.transform(fold_training[useful_columns]))[:, 1])\n",
    "    print(auc(fpr, tpr))\n",
    "    \n",
    "    clssfis.append(clssfi)\n",
    "    predict_results.append(clssfi.predict_proba(scaler.transform(training_df[useful_columns]))[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(training_df['label'], np.mean(predict_results, axis=0))\n",
    "print(auc(fpr, tpr))\n",
    "layer1_df['nn'] = np.mean(predict_results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(training_df['label'], layer1_df['nn'])\n",
    "print(auc(fpr, tpr))\n",
    "fpr, tpr, thresholds = roc_curve(training_df['label'], layer1_df['lgbm'])\n",
    "print(auc(fpr, tpr))\n",
    "fpr, tpr, thresholds = roc_curve(training_df['label'], layer1_df['nn']*0.4+layer1_df['lgbm']*0.6)\n",
    "print(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1_df['lgbm'] = gbm.predict(test_df[useful_columns].values, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_results = []\n",
    "for clssfi in clssfis:\n",
    "    predict_results.append(clssfi.predict_proba(scaler.transform(test_df[useful_columns]))[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1_df['nn'] = np.mean(predict_results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df = test_df.copy()\n",
    "result_df['ans'] = layer1_df['nn']*0.4+layer1_df['lgbm']*0.6\n",
    "result_df[['FileID', 'ans']].to_csv('df_log/228/result_0323.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
